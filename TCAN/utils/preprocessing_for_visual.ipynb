{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task, num_workers, ratio = [0.7, 0.2, 0.1]):\n",
    "    experiment_list = []\n",
    "    for filename in os.listdir(osp.join(dir_data_root, dataset_name, 'transcriptions')):\n",
    "        if os.path.isfile(osp.join(dir_data_root, dataset_name, 'transcriptions', filename)):\n",
    "            experiment_list.append(filename)\n",
    "    train_idx = int(len(experiment_list) * ratio[0])\n",
    "    valid_idx = int(len(experiment_list) * (ratio[0] +  ratio[1]))\n",
    "    if task == 'train':\n",
    "        experiment_list = experiment_list[:train_idx]\n",
    "    elif task == 'valid':\n",
    "        experiment_list = experiment_list[train_idx: valid_idx]\n",
    "    else:\n",
    "        experiment_list = experiment_list[valid_idx:]\n",
    "    _save_data(dir_data_root, dataset_name, task, seq_len, valid_len, experiment_list, num_workers)\n",
    "\n",
    "def _save_data(dir_data_root, dataset_name, task, seq_len, valid_len, experiment_list, num_workers):\n",
    "    processed_video_root = osp.join(dir_data_root, dataset_name, 'processed_video', task)\n",
    "    os.makedirs(processed_video_root, exist_ok=True)\n",
    "    label_all = []\n",
    "    indices_all = []\n",
    "    for idx, experiment in enumerate(experiment_list):\n",
    "        print('Experiment {}/{}'.format(idx, len(experiment_list)))\n",
    "        data_all_tokens, label_all_tokens = _get_single_experiment_data(dir_data_root, dataset_name, seq_len, valid_len, experiment)\n",
    "        # with Pool(processes=num_workers) as pool:\n",
    "        for idx, elem in enumerate(tqdm(data_all_tokens, ncols=80)):\n",
    "            # pool.apply_async(torch.save, (elem, osp.join(processed_video_root, '{}_{}_data.pt'.format(experiment.split('.')[0], idx))))\n",
    "            # torch.save(elem, osp.join(processed_video_root, '{}_{}_data.pt'.format(experiment.split('.')[0], idx)))\n",
    "            save_tensor_as_video(elem, osp.join(processed_video_root, '{}_{}_data.avi'.format(experiment.split('.')[0], idx)))\n",
    "        label_all.extend(label_all_tokens)\n",
    "        indices_all.extend(['{}_{}'.format(experiment.split('.')[0], i) for i in range(len(label_all_tokens))])\n",
    "    \n",
    "    # reindex label\n",
    "    unique_labels = torch.unique(torch.stack(label_all))\n",
    "    mapping = {label.item(): idx for idx, label in enumerate(unique_labels)}\n",
    "    # Apply the mapping to get the reindexed tensor\n",
    "    label_all = [torch.tensor(mapping[element.item()]) for element in label_all]\n",
    "    torch.save(label_all, osp.join(processed_video_root, 'labels.pt'))\n",
    "    torch.save(indices_all, osp.join(processed_video_root, 'label_indices.pt'))\n",
    "\n",
    "\n",
    "def _get_single_experiment_data(dir_data_root, dataset_name, seq_len, valid_len, suffix):\n",
    "    prefix = suffix.split('.')[0]\n",
    "\n",
    "    # load data\n",
    "    data_left = load_video_to_tensor(osp.join(dir_data_root, dataset_name, 'video', '{}_capture{}.avi'.format(prefix, '1')))\n",
    "    data_right = load_video_to_tensor(osp.join(dir_data_root, dataset_name, 'video', '{}_capture{}.avi'.format(prefix, '2')))\n",
    "\n",
    "    # load the labels\n",
    "    labels = pd.read_csv(osp.join(dir_data_root, dataset_name, 'transcriptions', suffix), sep='\\s+', header=None)\n",
    "    label_all = torch.zeros(max(data_left.size(0), data_right.size(0)))\n",
    "    for index, row in labels.iterrows():\n",
    "        label_all[row[0]:row[1] + 1] = int(row[2][1:])\n",
    "\n",
    "    label_all.to(torch.float32)\n",
    "\n",
    "    non_zero_indices = torch.nonzero(label_all, as_tuple=True)\n",
    "\n",
    "    data_left, data_right, label_all = data_left[non_zero_indices], data_right[non_zero_indices], label_all[non_zero_indices]\n",
    "\n",
    "    num_data = data_left.size(0) - seq_len\n",
    "    data_left_tokens, data_right_token, label_all_tokens = [], [], []\n",
    "    for i in range(num_data):\n",
    "        data_left_tokens.append(data_left[i:i + seq_len])\n",
    "        data_right_token.append(data_right[i:i + seq_len])\n",
    "        label_all_tokens.append(label_all[i + seq_len])\n",
    "\n",
    "    # to be discussed how to combine left-hand data and right-hand data\n",
    "    # data = torch.vstack([data_left, data_right])\n",
    "    # return data_left_tokens + data_right_token, label_all_tokens + label_all_tokens\n",
    "    return data_left_tokens, label_all_tokens\n",
    "\n",
    "def load_video_to_tensor(filename):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Convert list of frames into a numpy array and scale to [0, 1]\n",
    "    # frames_array = np.array(frames, dtype=np.float32) / 255.0\n",
    "    frames_array = np.array(frames)\n",
    "    frames_tensor = torch.from_numpy(frames_array)\n",
    "    # [number_of_frames, height, width, channels]\n",
    "    return frames_tensor\n",
    "\n",
    "def save_tensor_as_video(tensor, output_path, fps=30.0, codec='XVID'):\n",
    "    \"\"\"\n",
    "    Save a 4D PyTorch tensor as an AVI video file.\n",
    "\n",
    "    :param tensor: 4D tensor with shape [number of frames, height, width, channels]\n",
    "    :param output_path: Path to the output AVI file\n",
    "    :param fps: Frames per second for the output video\n",
    "    :param codec: Codec to be used for the output video\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy array and scale to 0-255\n",
    "    video_data = tensor.numpy().astype(np.uint8)\n",
    "\n",
    "    # Define video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (video_data.shape[2], video_data.shape[1]))\n",
    "\n",
    "    # Write frames to video\n",
    "    for frame in video_data:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Œ                                      | 26/1658 [02:05<2:10:54,  4.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_len \u001b[39m=\u001b[39m \u001b[39m80\u001b[39m\n\u001b[1;32m      5\u001b[0m num_workers \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m----> 7\u001b[0m generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, num_workers\u001b[39m=\u001b[39;49mnum_workers)\n\u001b[1;32m      8\u001b[0m generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39mnum_workers)\n\u001b[1;32m      9\u001b[0m generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, num_workers\u001b[39m=\u001b[39mnum_workers)\n",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(dir_data_root, dataset_name, seq_len, valid_len, task, num_workers, ratio)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     experiment_list \u001b[39m=\u001b[39m experiment_list[valid_idx:]\n\u001b[0;32m---> 24\u001b[0m _save_data(dir_data_root, dataset_name, task, seq_len, valid_len, experiment_list, num_workers)\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36m_save_data\u001b[0;34m(dir_data_root, dataset_name, task, seq_len, valid_len, experiment_list, num_workers)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39m# with Pool(processes=num_workers) as pool:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mfor\u001b[39;00m idx, elem \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(data_all_tokens, ncols\u001b[39m=\u001b[39m\u001b[39m80\u001b[39m)):\n\u001b[1;32m     35\u001b[0m         \u001b[39m# pool.apply_async(torch.save, (elem, osp.join(processed_video_root, '{}_{}_data.pt'.format(experiment.split('.')[0], idx))))\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         torch\u001b[39m.\u001b[39;49msave(elem, osp\u001b[39m.\u001b[39;49mjoin(processed_video_root, \u001b[39m'\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m_data.pt\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(experiment\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m], idx)))\n\u001b[1;32m     37\u001b[0m     label_all\u001b[39m.\u001b[39mextend(label_all_tokens)\n\u001b[1;32m     39\u001b[0m \u001b[39m# reindex label\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TCAN/lib/python3.8/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/TCAN/lib/python3.8/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage\u001b[39m.\u001b[39;49mdata_ptr(), num_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_data_root = '/data/mingxing/JIGSAWS'\n",
    "dataset_name = 'Knot_Tying'\n",
    "seq_len = 80\n",
    "valid_len = 80\n",
    "num_workers = 5\n",
    "\n",
    "generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task='train', num_workers=num_workers)\n",
    "generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task='valid', num_workers=num_workers)\n",
    "generate_dataset(dir_data_root, dataset_name, seq_len, valid_len, task='test', num_workers=num_workers)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
